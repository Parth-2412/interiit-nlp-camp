{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "474f63ed-74de-4db6-8f29-28e25909ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 17:08:56,743 [INFO] file_cache is only supported with oauth2client<4.0.0\n",
      "2025-08-17 17:08:56,745 [INFO] Google Search API initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import wikipedia\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "CX = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "service = None\n",
    "if API_KEY and CX:\n",
    "    try:\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=API_KEY)\n",
    "        logging.info(\"Google Search API initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to initialize Google Search API: {e}\")\n",
    "else:\n",
    "    logging.warning(\"Google Search API credentials not found. Skipping Google Search.\")\n",
    "\n",
    "\n",
    "def google_search(query: str, max_results: int = 5):\n",
    "    if not service:\n",
    "        raise RuntimeError(\"Google Search service not initialized.\")\n",
    "    try:\n",
    "        res = service.cse().list(q=query, cx=CX).execute()\n",
    "        return [\n",
    "            {\n",
    "                \"title\": item.get(\"title\", \"\"),\n",
    "                \"link\": item.get(\"link\", \"\"),\n",
    "                \"snippet\": item.get(\"snippet\", \"\")\n",
    "            }\n",
    "            for item in res.get(\"items\", [])[:max_results]\n",
    "        ]\n",
    "    except HttpError as e:\n",
    "        logging.error(f\"Google API error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected Google search error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 5):\n",
    "    \"\"\"Perform DuckDuckGo search with error handling.\"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": item.get(\"title\", \"\"),\n",
    "                \"link\": item.get(\"href\", \"\"),\n",
    "                \"snippet\": item.get(\"body\", \"\")\n",
    "            }\n",
    "            for item in results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"DuckDuckGo search error: {e}\")\n",
    "        raise\n",
    "        \n",
    "def wikipedia_search(query: str, max_results: int = 5):\n",
    "    \"\"\"Perform Wikipedia search with error handling.\"\"\"\n",
    "    try:\n",
    "        titles = wikipedia.search(query, results=max_results)\n",
    "        results = []\n",
    "        for title in titles:\n",
    "            try:\n",
    "                page = wikipedia.page(title, auto_suggest=False)\n",
    "                results.append({\n",
    "                    \"title\": page.title,\n",
    "                    \"link\": page.url,\n",
    "                    \"snippet\": page.summary[:300] + \"...\"  # limit snippet length\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Skipping problematic Wikipedia page '{title}': {e}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Wikipedia search error: {e}\")\n",
    "        raise\n",
    "\n",
    "def search(query: str):\n",
    "    try:\n",
    "        logging.info(f\"Trying Google search for: {query}\")\n",
    "        return google_search(query)\n",
    "    except Exception:\n",
    "        logging.info(f\"Falling back to DuckDuckGo search for: {query}\")\n",
    "        try:\n",
    "            return duckduckgo_search(query)\n",
    "        except Exception:\n",
    "            logging.info(f\"Falling back to Wikipedia search for: {query}\")\n",
    "            try:\n",
    "                return wikipedia_search(query)\n",
    "            except Exception:\n",
    "                logging.critical(\"All search engines failed.\")\n",
    "                raise RuntimeError(\"Search failed with all providers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d52a30c1-ced5-4ed8-ba75-6bd84ce1ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 17:13:42,337 [INFO] Searching for query: who is hakla\n",
      "2025-08-17 17:13:42,338 [INFO] Trying Google search for: who is hakla\n",
      "2025-08-17 17:13:42,654 [INFO] Got 5 search results\n",
      "2025-08-17 17:13:42,656 [INFO] Scraping 5 pages...\n",
      "2025-08-17 17:13:42,896 [INFO] Scraped 1 docs from https://www.hindustantimes.com/entertainment/bollywood/shah-rukh-khan-was-called-hakla-behind-his-back-by-many-stars-claims-singer-abhijeet-bhattacharya-101734775718319.html\n",
      "2025-08-17 17:13:43,518 [INFO] Scraped 1 docs from https://en.wikipedia.org/wiki/Haka\n",
      "2025-08-17 17:13:45,641 [INFO] Scraped 1 docs from https://www.reddit.com/r/TeenIndia/comments/1mepqyh/how_did_the_hakla_meme_even_start/\n",
      "2025-08-17 17:13:45,823 [INFO] Scraped 1 docs from https://timesofindia.indiatimes.com/viral-hakla-shah-rukh-khan-meme-explained-why-its-still-trending-despite-reported-takedown-attempts-by-srks-team/articleshow/123088001.cms\n",
      "2025-08-17 17:13:45,971 [INFO] Scraped 1 docs from https://vocal.media/geeks/what-is-the-hakla-srk-meme-and-why-it-s-going-viral\n",
      "2025-08-17 17:13:45,972 [INFO] Total docs scraped: 5\n",
      "2025-08-17 17:13:45,973 [INFO] Splitting documents into chunks...\n",
      "2025-08-17 17:13:45,976 [INFO] âœ… Created 49 chunks\n",
      "2025-08-17 17:13:45,976 [INFO] Building FAISS vectorstore with Google embeddings...\n"
     ]
    },
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 403 Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked. [reason: \"API_KEY_SERVICE_BLOCKED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"methodName\"\n  value: \"google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/1060660884177\"\n}\nmetadata {\n  key: \"apiName\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionDenied\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langchain_google_genai/embeddings.py:243\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.client.batch_embed_contents(\n\u001b[32m    244\u001b[39m         BatchEmbedContentsRequest(requests=requests, model=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    245\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1437\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m response = rpc(\n\u001b[32m   1438\u001b[39m     request,\n\u001b[32m   1439\u001b[39m     retry=retry,\n\u001b[32m   1440\u001b[39m     timeout=timeout,\n\u001b[32m   1441\u001b[39m     metadata=metadata,\n\u001b[32m   1442\u001b[39m )\n\u001b[32m   1444\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    295\u001b[39m     target,\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    297\u001b[39m     sleep_generator,\n\u001b[32m    298\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    299\u001b[39m     on_error=on_error,\n\u001b[32m    300\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m         exc,\n\u001b[32m    158\u001b[39m         deadline,\n\u001b[32m    159\u001b[39m         sleep_iter,\n\u001b[32m    160\u001b[39m         error_list,\n\u001b[32m    161\u001b[39m         predicate,\n\u001b[32m    162\u001b[39m         on_error,\n\u001b[32m    163\u001b[39m         exception_factory,\n\u001b[32m    164\u001b[39m         timeout,\n\u001b[32m    165\u001b[39m     )\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = target()\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mPermissionDenied\u001b[39m: 403 Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked. [reason: \"API_KEY_SERVICE_BLOCKED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"methodName\"\n  value: \"google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/1060660884177\"\n}\nmetadata {\n  key: \"apiName\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked.\"\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     98\u001b[39m     init_state = {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwho is hakla\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     final_state = app.invoke(init_state)\n\u001b[32m    100\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸŽ¯ Final Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_state[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(\n\u001b[32m   3027\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3028\u001b[39m     config,\n\u001b[32m   3029\u001b[39m     context=context,\n\u001b[32m   3030\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3031\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3032\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3033\u001b[39m     print_mode=print_mode,\n\u001b[32m   3034\u001b[39m     output_keys=output_keys,\n\u001b[32m   3035\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3036\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3037\u001b[39m     durability=durability,\n\u001b[32m   3038\u001b[39m     **kwargs,\n\u001b[32m   3039\u001b[39m ):\n\u001b[32m   3040\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3041\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2648\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2649\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2650\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2651\u001b[39m     schedule_task=loop.accept_push,\n\u001b[32m   2652\u001b[39m ):\n\u001b[32m   2653\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2654\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[32m   2655\u001b[39m         stream_mode, print_mode, subgraphs, stream.get, queue.Empty\n\u001b[32m   2656\u001b[39m     )\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     run_with_retry(\n\u001b[32m    163\u001b[39m         t,\n\u001b[32m    164\u001b[39m         retry_policy,\n\u001b[32m    165\u001b[39m         configurable={\n\u001b[32m    166\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    167\u001b[39m                 _call,\n\u001b[32m    168\u001b[39m                 weakref.ref(t),\n\u001b[32m    169\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    170\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    171\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    172\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    173\u001b[39m             ),\n\u001b[32m    174\u001b[39m         },\n\u001b[32m    175\u001b[39m     )\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m task.proc.invoke(task.input, config)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28mself\u001b[39m.func(*args, **kwargs)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mindex_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     60\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mBuilding FAISS vectorstore with Google embeddings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m embeddings = GoogleGenerativeAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mmodels/embedding-001\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m vectorstore = FAISS.from_documents(chunks, embeddings)\n\u001b[32m     63\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mâœ… Vectorstore ready\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {**state, \u001b[33m\"\u001b[39m\u001b[33mvectorstore\u001b[39m\u001b[33m\"\u001b[39m: vectorstore}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = embedding.embed_documents(texts)\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/nlpbootcamp/lib/python3.13/site-packages/langchain_google_genai/embeddings.py:247\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    243\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.client.batch_embed_contents(\n\u001b[32m    244\u001b[39m             BatchEmbedContentsRequest(requests=requests, model=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    248\u001b[39m     embeddings.extend([\u001b[38;5;28mlist\u001b[39m(e.values) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings])\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 403 Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked. [reason: \"API_KEY_SERVICE_BLOCKED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"methodName\"\n  value: \"google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/1060660884177\"\n}\nmetadata {\n  key: \"apiName\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"Requests to this API generativelanguage.googleapis.com method google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents are blocked.\"\n]",
      "During task with name 'index' and id '5bf28b89-8299-5731-e6e1-79bd2d4635c8'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import SoupStrainer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from typing import TypedDict, List\n",
    "\n",
    "# --- Logging setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SearchRAGState(TypedDict):\n",
    "    query: str\n",
    "    search_results: List[dict]\n",
    "    docs: List[str]\n",
    "    vectorstore: any\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def search_node(state: SearchRAGState) -> SearchRAGState:\n",
    "    logger.info(f\"Searching for query: {state['query']}\")\n",
    "    results = search(state[\"query\"])  \n",
    "    logger.info(f\"Got {len(results)} search results\")\n",
    "    return {**state, \"search_results\": results}\n",
    "\n",
    "\n",
    "def scrape_node(state: SearchRAGState) -> SearchRAGState:\n",
    "    logger.info(f\"Scraping {len(state['search_results'])} pages...\")\n",
    "    docs = []\n",
    "    for result in state[\"search_results\"]:\n",
    "        try:\n",
    "            loader = WebBaseLoader(\n",
    "                result[\"link\"], \n",
    "                bs_kwargs=dict(parse_only=SoupStrainer([\"p\", \"h1\", \"h2\", \"h3\"]))\n",
    "            )\n",
    "            page_docs = loader.load()\n",
    "            docs.extend(page_docs)\n",
    "            logger.info(f\"Scraped {len(page_docs)} docs from {result['link']}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping {result['link']} due to error: {e}\")\n",
    "    logger.info(f\"Total docs scraped: {len(docs)}\")\n",
    "    return {**state, \"docs\": docs}\n",
    "\n",
    "\n",
    "def index_node(state: SearchRAGState) -> SearchRAGState:\n",
    "    logger.info(\"Splitting documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(state[\"docs\"])\n",
    "    logger.info(f\"âœ… Created {len(chunks)} chunks\")\n",
    "\n",
    "    logger.info(\"Building FAISS vectorstore with Google embeddings...\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    logger.info(\"âœ… Vectorstore ready\")\n",
    "    return {**state, \"vectorstore\": vectorstore}\n",
    "\n",
    "\n",
    "def rag_node(state: SearchRAGState) -> SearchRAGState:\n",
    "    logger.info(\"Running RetrievalQA...\")\n",
    "    retriever = state[\"vectorstore\"].as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 5}\n",
    "    )\n",
    "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "    answer = qa.invoke(state[\"query\"])\n",
    "    logger.info(f\"Got answer from LLM\")\n",
    "    return {**state, \"answer\": answer}\n",
    "\n",
    "\n",
    "# --- Graph assembly ---\n",
    "workflow = StateGraph(SearchRAGState)\n",
    "\n",
    "workflow.add_node(\"search\", search_node)\n",
    "workflow.add_node(\"scrape\", scrape_node)\n",
    "workflow.add_node(\"index\", index_node)\n",
    "workflow.add_node(\"rag\", rag_node)\n",
    "\n",
    "workflow.set_entry_point(\"search\")\n",
    "workflow.add_edge(\"search\", \"scrape\")\n",
    "workflow.add_edge(\"scrape\", \"index\")\n",
    "workflow.add_edge(\"index\", \"rag\")\n",
    "workflow.add_edge(\"rag\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_state = {\"query\": \"who is thanos\"}\n",
    "    final_state = app.invoke(init_state)\n",
    "    logger.info(f\"ðŸŽ¯ Final Answer: {final_state['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90573641-0dc8-48cd-9603-75725e4853fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbootcamp",
   "language": "python",
   "name": "nlpbootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
